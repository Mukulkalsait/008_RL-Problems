# ğŸš¨ INCIDENT SCENARIO (LIVE ON-CALL)

**Time:** 02:17 AM
**You are Primary On-Call**

### PagerDuty Alert (CRITICAL)

> **Service:** `payments-api`
> **Environment:** Production
> **Alert:** `High Latency + Error Rate`
> **Details:**
>
> * P95 latency > **2.5s** (SLO: < 300ms) for **12 minutes**
> * Error rate **12%** (SLO: < 1%)
> * Affects **checkout flow**
> * Revenue-impacting service

---

### Architecture Context

* `payments-api`

  * Runs on **2 Linux VMs**
  * Behind **Nginx**
  * Uses **MySQL**
* Monitoring:

  * Prometheus + Grafana
* Logs:

  * systemd + app logs
* Last deployment:

  * **45 minutes ago**

---

## YOUR TASK (answer step-by-step)

Reply as if you are the on-call engineer.

### 1ï¸âƒ£ First 5 minutes

* What do you do **immediately**?
* What is the **first action** in PagerDuty?
* What do you check first and **why**?

---

### 2ï¸âƒ£ Investigation

You see this in **Grafana**:

* CPU: 35%
* Memory: 70%
* Disk: OK
* **DB connections: MAXED OUT**
* API pods/VMs are healthy
* Nginx logs show many `504 Gateway Timeout`

ğŸ‘‰ What does this suggest?

---

### 3ï¸âƒ£ Mitigation

* What is your **fastest mitigation**?
* What do you do to **restore service quickly**?

---

### 4ï¸âƒ£ Communication

* Who do you inform?
* What do you say (1â€“2 lines)?

---

### 5ï¸âƒ£ After recovery

* What do you do **after service is stable**?
* What would you add to **prevent this**?

---

### Rules

* Donâ€™t overthink
* Short, clear answers
* Think **restore first, root cause later**

---

When you reply, structure like this:

```
1. Immediate Actions:
2. Investigation:
3. Mitigation:
4. Communication:
5. Post-Incident Actions:
```

I will grade you like a **real SRE interview**.

----------------------------

# My Response

```bash

1. Immediate Actions:
a.pagerduty -> acknolegment that i will handel this alert
b.alert is real as its going for 12 minuits and evan if it is not falure the service is high priority payment-api so will go through process
c. last deployment was 45 minuts ago. so most probably bad push. 

2. Investigation: 
cpu is ok, memory is on slightly higher side. DB connections are maxed out. 
so DB overload either bad push, or High Trafic. but High trafic will affetc overall not just DB.

3. Mitigation:
a. if its backend /frontend srvice we can scale up but db must be single, so scaleup might not be the options, so we will try to find quickly find if we can directly fidn the culprits from logs, if no we will do roleback to the previous version. 

4. Communication:
on slack/insidence response page i will post the whole sinero, 
DB connection maxing out,
all timings,
happning from=> update version
important logs, and matrics,
may be some screenshots.
and quick fixes i did.

5. Post-Incident Actions:
to go root cause i will go through the logs carefully this time, 
if its on backend i will inform to the dev, if its on out side we will make up for it, 
also i will add 2 alerts, both on database connections 1 will be temperory which will trigger faster which will be there till we get through the root cause and 2nd will be a general trigger when the matrics stays longer like 12 min on higher side. 

```


----------------------------

# Resualt: 

Excellent. This is **solid juniorâ€“mid SRE level thinking** already.
Iâ€™ll **grade you**, then **correct + upgrade** your answers so they become **interview-ready**.

---

## âœ… Overall Grade: **7.5 / 10**

You are **absolutely not a beginner** in incident response thinking.

You:

* Acknowledged on-call correctly
* Prioritized service restoration
* Used metrics properly (DB connections = signal)
* Chose rollback (âœ… very SRE-correct)
* Thought about alert improvements (very good)

Now letâ€™s **tighten the weak points**.

---

# ğŸ”§ Corrections & Improvements (Important)

## 1ï¸âƒ£ Immediate Actions (On-Call)

### What you did right

* PagerDuty **ACK** âœ…
* Recognized revenue-impacting service âœ…
* Considered recent deploy âœ…

### What to improve (INTERVIEW KEY)

Instead of saying:

> â€œalert may be false but still checkâ€

Say this:

> â€œThe alert is valid because it breaches SLOs for 12 minutes on a revenue-critical service.â€

### Improved answer

```
- Acknowledge PagerDuty alert to stop escalation
- Confirm alert validity using Grafana SLO panels
- Check recent deploys and change history
- Start incident bridge / Slack thread immediately
```

ğŸ”¥ **Keyword interviewers want**: *â€œstop escalation, confirm SLO breachâ€*

---

## 2ï¸âƒ£ Investigation (Very Good, One Fix)

### Your logic

> DB connections maxed out â†’ bad push or traffic

Correct ğŸ‘
But this line is slightly weak:

> â€œHigh traffic will affect overall not just DBâ€

âŒ Not always true.

### Better explanation

```
DB connection exhaustion suggests:
- Connection leak
- Missing connection pooling
- Increased DB queries per request after deploy
```

ğŸ”¥ **Interview keyword**: *connection leak*

---

## 3ï¸âƒ£ Mitigation (Good Choice, Slight Adjustment)

### You chose rollback âœ…

Thatâ€™s **100% correct**

But mitigation must be **fastest guaranteed restore**.

### Stronger mitigation sequence

```
- Temporarily increase DB max connections (if safe)
- Restart affected application instances
- Roll back last deployment immediately
```

> Rollback beats debugging at 2 AM

ğŸ”¥ **This sentence alone passes SRE interviews**.

---

## 4ï¸âƒ£ Communication (Good, Make It SRE-Style)

Your communication was **too long** for incident time.

### Correct incident update format

```
[INCIDENT] payments-api
Impact: Checkout failures (12% error rate)
Cause: Suspected DB connection exhaustion post-deploy
Action: Rolled back deployment, monitoring recovery
Next Update: 10 minutes
```

ğŸ”¥ Short. Clear. No screenshots during outage.

---

## 5ï¸âƒ£ Post-Incident Actions (Very Strong)

This part was **excellent**.

### What you did right

* Root cause later âœ…
* Involved dev team âœ…
* Alert improvements âœ…

### Make it elite SRE

Add **one sentence**:

```
Create postmortem with:
- Timeline
- Root cause
- Corrective actions
- Preventive actions
```

ğŸ”¥ **Postmortem is sacred in SRE**.

---

# ğŸ” Security Attacks You MUST Know (Answer to your earlier question)

You should know these at **high level**:

### Network / Availability

* DoS / DDoS
* SYN flood
* Slowloris

### Application

* SQL Injection
* Command Injection
* Path Traversal
* XSS

### Auth

* Brute force
* Credential stuffing
* Privilege escalation

### Infra

* SSH brute force
* Exposed ports
* Misconfigured IAM
* Secrets in env/files

You **do NOT need deep hacking**, only:

> *Detection + mitigation mindset*

---

# ğŸ§  Reality Check (Important)

You said:

> â€œI have no idea about on-call, incidentsâ€

That is **false** now.

You already think like:

* Junior SRE
* Linux production engineer

You just lacked **vocabulary + structure**.

---

## Final Truth (No Motivation Talk)

If you explain incidents like this in interviews:

* Linux Admin â†’ âœ…
* Junior DevOps â†’ âœ…
* SRE (Junior/Mid) â†’ borderline but possible

---

If you want, next we can:

1. Do **one security incident**
2. Do **disk-full outage**
3. Do **memory leak incident**
4. Turn this into **resume bullet points**

Say **which one next**.
