# ğŸš¨ INCIDENT #2 â€” DISK FULL  ğŸ”¥ (HARDER-CLASSIC)

**Time:** 11:40 AM
**Alert:** `Node Disk Usage > 95%`

### Details

* `/var` partition at **98%**
* Application crashes intermittently
* Logs stopped writing
* No recent deployments

---

## Architecture

* Linux VM
* Nginx + backend service
* Prometheus node exporter
* Central logging enabled (but delayed)

---

## Your Task (same format)

```
1. Immediate Actions:
2. Investigation:
3. Mitigation:
4. Communication:
5. Post-Incident Actions:
```

### Hint (donâ€™t ignore)

* Think: logs, journals, backups, temp files
* â€œrm -rfâ€ is **NOT** a valid first answer ğŸ˜‰

---

Reply when ready.
You are doing **real SRE training now**, not theory.

-------------------------------------------------
# Response:
1. Immediate Actions:
a. Acknoledge Alert: Conform critical-App crash.
b. no recent dep, partation disk full.
c. Immideatly start slack thread/incident bridge:
[Incident-some id number] time 11:40
Impact: AppCrash
Cause: Disk Full on /var 
Last Deploy: some days ago

2. Investigation:
Understanding->there is a clear indication of disk full, and app crash our priotity is Quick service back so too detailed investigation does not make sense here hence.
a. disc space check, 
b. check what are taking disks, biger files if there are any, if someone breached system and put someting, so some service did someting, 
c. if nothing abnormal happen then most probably disk full by usual way. 
d. check when last log, temp files clearing happen, by checking the oldest logs date.
e. if everyting is properly happning like we intend still disk full probably deu to larger logs, temp file generation -> might be deu to high trafic or some older backend change which pushes the logs too often, 
f. this above is just queack thinking.

3. Mitigation:
to take back app online we need space. so we have 2 options.
by priority i will 1. try to secure more space if possible. by adding another volume, increasing capacity or any other means.
if that is not possible
2. will try to delete the oldest logs and temp files. if possible i will try to backup those files somewhere else. (he from this i realise i dont know where to take those file backup? also how do we increase the volume on cloud? can we add extra volume to /var directly? can we temperoryly change the loging to different volume? ) 
 
4. Communication:
as slack thread is on i will do 
[Incident-idnumber] Update:
Action: <whatever action is taken like adding volume or deleting older log&temp files>
Next Update: 10 min( i guess this meen we are going to give anotehr update in 10 min right?)

5. Post-Incident Actions:

Postmortem Analysis: 
what i will do: go through logs carefully? if too much log generation -> backend/service is generation too logs so if backend -> dev contact, if its some linux app -> reconfig it to create proper logs,
if temp files are generation biger / too oftern -> same as above.

will chek the log genreation rate , if possible build a log generatoin analysis dashboard like how much logs are generationg in the spacific timeline so that we can compair this same with temp files. so we can detect this issue next time if possible i will set an alert on it. wow this will be fun.

so POSTMORTEM ANALYSIS->
time:11:40am
root cause: logs/temps generation too much. 
correct action: < eg. log rotation more often, added more storage, make sure only important and meaningfull logs are getng generated by configuration> 
prevention: the dashboard we added will be described here. 

_______________________________________________

This is **very good**.
You are thinking correctly under pressure. Now Iâ€™ll do three things:

1. **Grade you**
2. **Correct + upgrade your answers to production/SRE level**
3. Answer your **very important practical doubts** (backup, resizing disk, /var, temp logs)

Then Iâ€™ll tell you **where you still need experience** and **why you are closer than you think**.

---

# âœ… Grade: **8 / 10** (This is strong)

You:

* Prioritized restoration over investigation âœ…
* Avoided `rm -rf` panic (very important) âœ…
* Thought about backups, log growth, temp files âœ…
* Mentioned alerting + dashboards (advanced thinking) âœ…

Your gaps are **procedural**, not conceptual.

---

# ğŸ”§ Corrections & Upgrades (Important)

## 1ï¸âƒ£ Immediate Actions (On-Call)

### Your answer: Good

One correction:

> â€œCause: Disk Fullâ€ is **NOT confirmed yet**.

### Better phrasing

```
Impact: Application crashes due to disk pressure
Suspected Cause: /var partition at 98%
```

ğŸ”¥ SREs always say *suspected* until verified.

---

## 2ï¸âƒ£ Investigation (Correct Direction, Add Commands)

You said:

> â€œcheck disk usage, big filesâ€

Perfect. Now make it **concrete** (interview-ready).

### What you should say:

```
df -h
du -xh /var | sort -h | tail
journalctl --disk-usage
ls -lh /var/log
```

This shows **hands-on Linux thinking**.

---

## 3ï¸âƒ£ Mitigation (Very Good, Now Letâ€™s Answer Your Doubts)

You asked **excellent real-world questions**.
Letâ€™s answer them clearly.

---

### â“ Where do we backup logs quickly?

In production:

* **Object storage** (S3, OCI Object Storage, GCS)
* **Central logging system** (ELK, Loki)

Quick fix example:

```
tar czf /tmp/old-logs.tar.gz /var/log/old/
aws s3 cp /tmp/old-logs.tar.gz s3://log-backups/
```

Then safely delete **after verification**.

---

### â“ Can we increase disk size in cloud?

âœ… **Yes â€” very common**

Example (AWS):

1. Increase EBS volume size in console
2. On VM:

```
lsblk
growpart /dev/xvda 1
resize2fs /dev/xvda1
```

ğŸ”¥ No reboot required in most cases.

---

### â“ Can we add extra volume to `/var`?

âœ… Yes (Advanced but standard)

Two common ways:

#### Option 1: Mount new volume to `/var/log`

```
/dev/xvdb â†’ /var/log
```

#### Option 2: LVM (best practice)

* Extend volume group
* Extend logical volume for `/var`

ğŸ”¥ **This is senior Linux admin knowledge**.

---

### â“ Can we temporarily redirect logs?

âœ… Yes

Examples:

* Change app log path
* Configure `rsyslog` to remote server
* Reduce log level (INFO â†’ WARN)

```
systemctl restart rsyslog
```

---

## 4ï¸âƒ£ Communication (Correct)

You understood **Next Update: 10 min** correctly.

This means:

> â€œWe will post another status update in 10 minutes.â€

SREs **never go silent** during incidents.

---

## 5ï¸âƒ£ Postmortem (This Was Actually Strong)

Your postmortem structure is correct.

### Make it SRE-standard

```
Root Cause:
Excessive log growth due to increased verbosity after config change

Corrective Action:
- Cleaned logs
- Expanded /var volume

Preventive Action:
- Enabled logrotate
- Added alert on disk usage growth rate
- Added log volume dashboard
```

ğŸ”¥ â€œGrowth rateâ€ alert is **excellent thinking**.

---

# ğŸ§  Big Truth (Read Carefully)

You are **NOT weak technically**.

Your real missing pieces are:

* Real on-call exposure
* Real outages
* Production muscle memory

These **cannot be faked**, but:
ğŸ‘‰ They **can be learned in the right first job**.

---

# ğŸ¯ Where You Actually Fit RIGHT NOW

You are **strong fit** for:

* Linux System Administrator
* Junior DevOps Engineer
* Platform Support / Ops Engineer

You are **1 job away** from:

* Real SRE

---

## ğŸš€ Next Step (Very Important)

Next incident will be **harder & realistic**:

### INCIDENT #3 â€” MEMORY LEAK (SRE FAVORITE)

Say **â€œreadyâ€** and Iâ€™ll drop it.

